{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Semiconductor Manufacturing Process Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "This project builds a logistic regression model to predict whether a manufactured part passes or fails.\n",
    "\n",
    "Source: https://www.kaggle.com/saurabhbagchi/fmst-semiconductor-manufacturing-project\n",
    "\n",
    "A complex modern semiconductor manufacturing process is normally under constant surveillance via the monitoring of signals/variables collected from sensors and or process measurement points. However, not all of these signals are equally valuable in a specific monitoring system. The measured signals contain a combination of useful information, irrelevant information as well as noise. Engineers typically have a much larger number of signals than are actually required. If we consider each type of signal as a feature, then feature selection may be applied to identify the most relevant signals. The Process Engineers may then use these signals to determine key factors contributing to yield excursions downstream in the process. This will enable an increase in process throughput, decreased time to learning, and reduce per-unit production costs. These signals can be used as features to predict the yield type. And by analyzing and trying out different combinations of features, essential signals that are impacting the yield type can be identified.\n",
    "\n",
    "Dataset: SemiconductorManufacturingProcessDataset.csv (on Canvas)\n",
    "\n",
    "Later, we will learn how to apply PCA (Principal Component Analyses) for feature selection; then we will apply ANN to predict the Pass/Fail. in this exercise our objective is to repeat the same steps we did above for Supplier Data: Cleaning & Scaling Data, Encode Categorical Data, Split the Data to Training & Test Sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N-qiINBQSK2g"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WwEPNDWySTKm"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('SemiconductorManufacturingProcessDataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the Dataset in a Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Sensor 1</th>\n",
       "      <th>Sensor 2</th>\n",
       "      <th>Sensor 3</th>\n",
       "      <th>Sensor 4</th>\n",
       "      <th>Sensor 5</th>\n",
       "      <th>Sensor 6</th>\n",
       "      <th>Sensor 7</th>\n",
       "      <th>Sensor 8</th>\n",
       "      <th>Sensor 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Sensor 429</th>\n",
       "      <th>Sensor 430</th>\n",
       "      <th>Sensor 431</th>\n",
       "      <th>Sensor 432</th>\n",
       "      <th>Sensor 433</th>\n",
       "      <th>Sensor 434</th>\n",
       "      <th>Sensor 435</th>\n",
       "      <th>Sensor 436</th>\n",
       "      <th>Sensor 437</th>\n",
       "      <th>Pass/Fail</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/19/2008 11:55</td>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>...</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/19/2008 12:32</td>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/19/2008 13:17</td>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>...</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7/19/2008 14:43</td>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>...</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7/19/2008 15:22</td>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>...</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>10/16/2008 15:13</td>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>1.3424</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>...</td>\n",
       "      <td>11.7256</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>10/16/2008 20:49</td>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>1.4333</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>...</td>\n",
       "      <td>17.8379</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>10/17/2008 5:26</td>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>17.7267</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>43.5231</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>10/17/2008 6:01</td>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>1.4622</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>...</td>\n",
       "      <td>19.2104</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>93.4941</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>10/17/2008 6:07</td>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>22.9183</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>137.7844</td>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 439 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Time  Sensor 1  Sensor 2   Sensor 3   Sensor 4  Sensor 5  \\\n",
       "0      7/19/2008 11:55   3030.93   2564.00  2187.7333  1411.1265    1.3602   \n",
       "1      7/19/2008 12:32   3095.78   2465.14  2230.4222  1463.6606    0.8294   \n",
       "2      7/19/2008 13:17   2932.61   2559.94  2186.4111  1698.0172    1.5102   \n",
       "3      7/19/2008 14:43   2988.72   2479.90  2199.0333   909.7926    1.3204   \n",
       "4      7/19/2008 15:22   3032.24   2502.87  2233.3667  1326.5200    1.5334   \n",
       "...                ...       ...       ...        ...        ...       ...   \n",
       "1562  10/16/2008 15:13   2899.41   2464.36  2179.7333  3085.3781    1.4843   \n",
       "1563  10/16/2008 20:49   3052.31   2522.55  2198.5667  1124.6595    0.8763   \n",
       "1564   10/17/2008 5:26   2978.81   2379.78  2206.3000  1110.4967    0.8236   \n",
       "1565   10/17/2008 6:01   2894.92   2532.01  2177.0333  1183.7287    1.5726   \n",
       "1566   10/17/2008 6:07   2944.92   2450.76  2195.4444  2914.1792    1.5978   \n",
       "\n",
       "      Sensor 6  Sensor 7  Sensor 8  Sensor 9  ...  Sensor 429  Sensor 430  \\\n",
       "0      97.6133    0.1242    1.5005    0.0162  ...     14.9509      0.5005   \n",
       "1     102.3433    0.1247    1.4966   -0.0005  ...     10.9003      0.5019   \n",
       "2      95.4878    0.1241    1.4436    0.0041  ...      9.2721      0.4958   \n",
       "3     104.2367    0.1217    1.4882   -0.0124  ...      8.5831      0.4990   \n",
       "4     100.3967    0.1235    1.5031   -0.0031  ...     10.9698      0.4800   \n",
       "...        ...       ...       ...       ...  ...         ...         ...   \n",
       "1562   82.2467    0.1248    1.3424   -0.0045  ...     11.7256      0.4988   \n",
       "1563   98.4689    0.1205    1.4333   -0.0061  ...     17.8379      0.4975   \n",
       "1564   99.4122    0.1208       NaN       NaN  ...     17.7267      0.4987   \n",
       "1565   98.7978    0.1213    1.4622   -0.0072  ...     19.2104      0.5004   \n",
       "1566   85.1011    0.1235       NaN       NaN  ...     22.9183      0.4987   \n",
       "\n",
       "      Sensor 431  Sensor 432  Sensor 433  Sensor 434  Sensor 435  Sensor 436  \\\n",
       "0         0.0118      0.0035      2.3630         NaN         NaN         NaN   \n",
       "1         0.0223      0.0055      4.4447      0.0096      0.0201      0.0060   \n",
       "2         0.0157      0.0039      3.1745      0.0584      0.0484      0.0148   \n",
       "3         0.0103      0.0025      2.0544      0.0202      0.0149      0.0044   \n",
       "4         0.4766      0.1045     99.3032      0.0202      0.0149      0.0044   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1562      0.0143      0.0039      2.8669      0.0068      0.0138      0.0047   \n",
       "1563      0.0131      0.0036      2.6238      0.0068      0.0138      0.0047   \n",
       "1564      0.0153      0.0041      3.0590      0.0197      0.0086      0.0025   \n",
       "1565      0.0178      0.0038      3.5662      0.0262      0.0245      0.0075   \n",
       "1566      0.0181      0.0040      3.6275      0.0117      0.0162      0.0045   \n",
       "\n",
       "      Sensor 437  Pass/Fail  \n",
       "0            NaN       Pass  \n",
       "1       208.2045       Pass  \n",
       "2        82.8602       Fail  \n",
       "3        73.8432       Pass  \n",
       "4        73.8432       Pass  \n",
       "...          ...        ...  \n",
       "1562    203.1720       Pass  \n",
       "1563    203.1720       Pass  \n",
       "1564     43.5231       Pass  \n",
       "1565     93.4941       Pass  \n",
       "1566    137.7844       Pass  \n",
       "\n",
       "[1567 rows x 439 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(dataset)\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Review of the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1567 entries, 0 to 1566\n",
      "Columns: 439 entries, Time to Pass/Fail\n",
      "dtypes: float64(437), object(2)\n",
      "memory usage: 5.2+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate The Input and Output\n",
    "Here, we put the independent variables in X and the dependent variable in y. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 1:438].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing the Input Data in a Table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "hCsz2yCebe1R",
    "outputId": "1e4cc568-4e51-4b38-9d46-4aa3f15204be"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6765</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0148</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1065</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0952</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>-0.0033</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7585</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6597</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>2899.41</td>\n",
       "      <td>2464.36</td>\n",
       "      <td>2179.7333</td>\n",
       "      <td>3085.3781</td>\n",
       "      <td>1.4843</td>\n",
       "      <td>82.2467</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>1.3424</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>-0.0057</td>\n",
       "      <td>...</td>\n",
       "      <td>1.4879</td>\n",
       "      <td>11.7256</td>\n",
       "      <td>0.4988</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>2.8669</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>3052.31</td>\n",
       "      <td>2522.55</td>\n",
       "      <td>2198.5667</td>\n",
       "      <td>1124.6595</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>98.4689</td>\n",
       "      <td>0.1205</td>\n",
       "      <td>1.4333</td>\n",
       "      <td>-0.0061</td>\n",
       "      <td>-0.0093</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0187</td>\n",
       "      <td>17.8379</td>\n",
       "      <td>0.4975</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>2.6238</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>203.1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>2978.81</td>\n",
       "      <td>2379.78</td>\n",
       "      <td>2206.3000</td>\n",
       "      <td>1110.4967</td>\n",
       "      <td>0.8236</td>\n",
       "      <td>99.4122</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2237</td>\n",
       "      <td>17.7267</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>3.0590</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>43.5231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>2894.92</td>\n",
       "      <td>2532.01</td>\n",
       "      <td>2177.0333</td>\n",
       "      <td>1183.7287</td>\n",
       "      <td>1.5726</td>\n",
       "      <td>98.7978</td>\n",
       "      <td>0.1213</td>\n",
       "      <td>1.4622</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7085</td>\n",
       "      <td>19.2104</td>\n",
       "      <td>0.5004</td>\n",
       "      <td>0.0178</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>3.5662</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>93.4941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>2944.92</td>\n",
       "      <td>2450.76</td>\n",
       "      <td>2195.4444</td>\n",
       "      <td>2914.1792</td>\n",
       "      <td>1.5978</td>\n",
       "      <td>85.1011</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.2878</td>\n",
       "      <td>22.9183</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>3.6275</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>137.7844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 437 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0        1          2          3       4         5       6    \\\n",
       "0     3030.93  2564.00  2187.7333  1411.1265  1.3602   97.6133  0.1242   \n",
       "1     3095.78  2465.14  2230.4222  1463.6606  0.8294  102.3433  0.1247   \n",
       "2     2932.61  2559.94  2186.4111  1698.0172  1.5102   95.4878  0.1241   \n",
       "3     2988.72  2479.90  2199.0333   909.7926  1.3204  104.2367  0.1217   \n",
       "4     3032.24  2502.87  2233.3667  1326.5200  1.5334  100.3967  0.1235   \n",
       "...       ...      ...        ...        ...     ...       ...     ...   \n",
       "1562  2899.41  2464.36  2179.7333  3085.3781  1.4843   82.2467  0.1248   \n",
       "1563  3052.31  2522.55  2198.5667  1124.6595  0.8763   98.4689  0.1205   \n",
       "1564  2978.81  2379.78  2206.3000  1110.4967  0.8236   99.4122  0.1208   \n",
       "1565  2894.92  2532.01  2177.0333  1183.7287  1.5726   98.7978  0.1213   \n",
       "1566  2944.92  2450.76  2195.4444  2914.1792  1.5978   85.1011  0.1235   \n",
       "\n",
       "         7       8       9    ...     427      428     429     430     431  \\\n",
       "0     1.5005  0.0162 -0.0034  ...  1.6765  14.9509  0.5005  0.0118  0.0035   \n",
       "1     1.4966 -0.0005 -0.0148  ...  1.1065  10.9003  0.5019  0.0223  0.0055   \n",
       "2     1.4436  0.0041  0.0013  ...  2.0952   9.2721  0.4958  0.0157  0.0039   \n",
       "3     1.4882 -0.0124 -0.0033  ...  1.7585   8.5831  0.4990  0.0103  0.0025   \n",
       "4     1.5031 -0.0031 -0.0072  ...  1.6597  10.9698  0.4800  0.4766  0.1045   \n",
       "...      ...     ...     ...  ...     ...      ...     ...     ...     ...   \n",
       "1562  1.3424 -0.0045 -0.0057  ...  1.4879  11.7256  0.4988  0.0143  0.0039   \n",
       "1563  1.4333 -0.0061 -0.0093  ...  1.0187  17.8379  0.4975  0.0131  0.0036   \n",
       "1564     NaN     NaN     NaN  ...  1.2237  17.7267  0.4987  0.0153  0.0041   \n",
       "1565  1.4622 -0.0072  0.0032  ...  1.7085  19.2104  0.5004  0.0178  0.0038   \n",
       "1566     NaN     NaN     NaN  ...  1.2878  22.9183  0.4987  0.0181  0.0040   \n",
       "\n",
       "          432     433     434     435       436  \n",
       "0      2.3630     NaN     NaN     NaN       NaN  \n",
       "1      4.4447  0.0096  0.0201  0.0060  208.2045  \n",
       "2      3.1745  0.0584  0.0484  0.0148   82.8602  \n",
       "3      2.0544  0.0202  0.0149  0.0044   73.8432  \n",
       "4     99.3032  0.0202  0.0149  0.0044   73.8432  \n",
       "...       ...     ...     ...     ...       ...  \n",
       "1562   2.8669  0.0068  0.0138  0.0047  203.1720  \n",
       "1563   2.6238  0.0068  0.0138  0.0047  203.1720  \n",
       "1564   3.0590  0.0197  0.0086  0.0025   43.5231  \n",
       "1565   3.5662  0.0262  0.0245  0.0075   93.4941  \n",
       "1566   3.6275  0.0117  0.0162  0.0045  137.7844  \n",
       "\n",
       "[1567 rows x 437 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Quick Check of the Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eYrOQ43XcJR3",
    "outputId": "e0873b2a-3b08-4bab-ef0d-15b88858ca44"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1566</th>\n",
       "      <td>Pass</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1567 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "0     Pass\n",
       "1     Pass\n",
       "2     Fail\n",
       "3     Pass\n",
       "4     Pass\n",
       "...    ...\n",
       "1562  Pass\n",
       "1563  Pass\n",
       "1564  Pass\n",
       "1565  Pass\n",
       "1566  Pass\n",
       "\n",
       "[1567 rows x 1 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## Taking care of missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c93k7ipkSexq"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X)\n",
    "X = imputer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "3UgLdMS_bjq_",
    "outputId": "254af4e0-681e-47f5-aaa7-b9c6f43258e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.03093000e+03 2.56400000e+03 2.18773330e+03 ... 1.64749042e-02\n",
      "  5.28333333e-03 9.96700663e+01]\n",
      " [3.09578000e+03 2.46514000e+03 2.23042220e+03 ... 2.01000000e-02\n",
      "  6.00000000e-03 2.08204500e+02]\n",
      " [2.93261000e+03 2.55994000e+03 2.18641110e+03 ... 4.84000000e-02\n",
      "  1.48000000e-02 8.28602000e+01]\n",
      " ...\n",
      " [2.97881000e+03 2.37978000e+03 2.20630000e+03 ... 8.60000000e-03\n",
      "  2.50000000e-03 4.35231000e+01]\n",
      " [2.89492000e+03 2.53201000e+03 2.17703330e+03 ... 2.45000000e-02\n",
      "  7.50000000e-03 9.34941000e+01]\n",
      " [2.94492000e+03 2.45076000e+03 2.19544440e+03 ... 1.62000000e-02\n",
      "  4.50000000e-03 1.37784400e+02]]\n"
     ]
    }
   ],
   "source": [
    "# A quick check\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5hwuVddlSwVi"
   },
   "source": [
    "We don't have any categorical data to encode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XgHCShVyTOYY"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FyhY8-gPpFCa",
    "outputId": "7f76ef29-5423-4c3e-cf69-45fbc366a997"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# a quick check\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AxjSUXFQqo-3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## Splitting the Dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pXgA6CzlqbCl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "GuwQhFdKrYTM",
    "outputId": "de1e527f-c229-4daf-e7c5-ea9d2485148d"
   },
   "outputs": [],
   "source": [
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "GuwQhFdKrYTM",
    "outputId": "de1e527f-c229-4daf-e7c5-ea9d2485148d"
   },
   "outputs": [],
   "source": [
    "# print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "GuwQhFdKrYTM",
    "outputId": "de1e527f-c229-4daf-e7c5-ea9d2485148d"
   },
   "outputs": [],
   "source": [
    "# print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "colab_type": "code",
    "id": "GuwQhFdKrYTM",
    "outputId": "de1e527f-c229-4daf-e7c5-ea9d2485148d"
   },
   "outputs": [],
   "source": [
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=250)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Initialize the Logistic Regressor\n",
    "LR_model = LogisticRegression()\n",
    "LR_model.max_iter = 250\n",
    "LR_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# produce predictions\n",
    "y_pred = LR_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average model prediction accuracy: 91.09%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(LR_model, X_test, y_test, scoring='accuracy', cv=5)\n",
    "average_score = np.average(scores)\n",
    "\n",
    "print('\\nAverage model prediction accuracy: {:.2%}\\n'.format(average_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for Logistic Regressor\n",
      "[[  4  18]\n",
      " [ 17 275]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix for Logistic Regressor\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[-4.10316093e+01 -3.11294753e+01 -7.49156139e+01 -3.17650901e+00\n -7.57019359e-02 -1.61407726e+01 -1.76669476e+01 -7.88195053e+00\n -2.30037579e+01  3.41914665e+01 -5.45982666e+01 -6.12619774e+01\n -2.73893518e+00 -2.40282052e+01 -4.24838459e+00 -6.78487687e+01\n -6.80796596e+01 -5.69238045e+01 -1.13104029e+02  8.97187487e+00\n -9.14385944e+00  2.76375286e+00  1.02992075e-01 -5.06466180e+00\n -8.43917167e+00 -4.95493355e+00 -2.01295949e+01 -7.25879649e+00\n -2.44824574e+01 -7.52967779e+00 -4.21515316e+01 -6.80091502e+00\n -4.31178902e+01 -2.51901071e+01 -4.14942584e+01 -2.19079096e+02\n -1.96051262e+02 -6.57562171e+01 -2.82917802e+00 -1.38167606e+00\n -5.71286922e+01 -5.91807529e+01 -1.73937402e+01 -6.03188001e+01\n -6.09747298e-01 -3.09081875e+01 -7.31797179e+01 -2.57436269e+00\n -1.08627620e+02 -1.02399836e+02 -1.11074175e+02 -2.26493241e+02\n -2.86061197e+02 -5.16976586e+01 -3.24513383e-01 -5.91145926e+01\n -4.05972685e+01 -1.34723483e+01 -2.01306083e+00 -4.23139809e+00\n -3.84754573e+00 -6.09215626e+01 -5.46475432e-02 -3.47818110e+01\n -6.50097006e+01 -3.33925082e+00 -1.16497549e+01 -3.78217082e+01\n -1.99264901e+01  2.61400362e+01 -3.75875301e+00 -3.26843434e+01\n  9.41723418e+01 -3.45828917e+01  1.82762949e+01 -6.69343323e+00\n -1.56714388e+01 -3.38977840e+02 -4.17218233e+01 -4.87279529e+01\n -3.37839082e+01 -4.23123793e+00 -2.26530547e+01 -4.63202319e+00\n -6.80008709e+01  1.26317994e+02 -8.97216448e+03  1.28975197e+04\n  6.29013074e+00 -9.17502967e-01  5.52813047e+00 -5.39347631e+03\n -1.91184413e+03  5.19742846e+00  6.60420512e+01 -3.95053589e+02\n  6.85150165e+01 -1.28394190e+02  4.20138850e-02  1.29749465e+00\n -1.28690941e+01 -6.17977909e+01 -1.52735259e+01 -3.57290982e+02\n -9.06234214e+00 -3.90304895e+01 -1.30725279e+02 -5.42155042e+01\n -1.93197750e+02 -3.62801822e+00 -2.03201234e+02 -1.36865321e+02\n -2.58715857e+00 -1.57388741e+01 -2.13554771e+00 -1.36247759e+01\n  1.02177466e+00 -2.42560889e+01  3.76595527e+03 -4.51970768e+01\n -1.54040277e+02 -1.30785552e+01 -2.06504426e+00 -2.59545508e+00\n -2.34626480e+00 -4.58844646e+00 -1.59249003e+00 -5.17916309e-02\n -1.89081689e+00 -2.90818285e+02 -4.17034327e+00 -3.30107473e+01\n -2.75170039e+00 -9.23776875e+00 -4.46438150e-01 -1.42928974e+00\n -4.55227430e-01 -5.22512485e-02 -4.86438377e+01 -1.38781680e+00\n -5.87077468e-01  6.09995257e+00 -8.97616918e-01 -9.67277763e-01\n -9.59337183e-01 -7.32507428e-01 -7.35829751e+00 -4.92985133e-01\n -1.53735123e+00 -1.60514073e+00 -7.93229598e-01 -5.81531207e-01\n -6.55677066e-01 -5.59355417e+00 -1.34347462e+00 -7.75361107e+00\n  8.54465449e-02 -7.75306538e+00 -1.35929688e+01 -2.03798983e+01\n  6.21319431e+00 -5.83205450e+00  1.34439285e+00 -2.57100720e+00\n -3.71308137e+00 -2.71796659e+00 -1.01938410e+00 -2.01479888e+00\n -1.99985487e+00 -1.08145796e+00 -5.73194796e-01 -1.98293851e+00\n  5.58015979e-01 -7.01134119e-01 -2.09277747e+00 -1.68820393e+00\n -7.18697613e-01 -1.70875354e+00 -5.72414940e-02 -8.12620552e-01\n -1.16881877e+00 -2.63226170e+00  1.87377296e+01  3.56539745e+01\n -1.52288851e+01 -6.65661296e+00  4.77243593e+00 -3.53599610e+00\n  4.85032750e+01 -1.53342640e+01 -2.91338090e+00 -4.68651190e+02\n -3.98538187e+01 -1.47571751e+00 -2.24390442e+00 -7.20564301e+00\n -2.44558880e+00  6.66395036e+00 -6.62663390e+01  3.49255398e+01\n  5.39393011e+02 -1.21870189e+01 -1.57277972e+00 -2.01858165e+00\n -2.46080775e-01 -7.25012819e-02 -6.77989253e+00  9.51433359e+01\n  5.64371927e+00 -1.95352433e+01 -2.82984534e+00 -2.27011764e+00\n -3.58284144e+00 -2.65903728e+00 -2.32668693e+00 -5.13074681e+00\n -1.60647991e+00 -5.20037638e-02 -2.06267098e+00 -1.89612768e+03\n -1.89578954e+01 -1.48998813e+02  4.30814142e+01 -2.76711024e+01\n -4.63226833e-01  1.00245554e-01 -4.44867206e-01 -5.28670227e-02\n -1.43646130e+02 -1.35638527e+00 -8.45553589e-01  1.14434280e+01\n -8.40887555e-01 -8.93602758e-01 -9.50396721e-01 -7.26305723e-01\n -1.13915724e+01 -6.23076809e-01 -2.09467857e+00  1.99013418e-01\n  1.29358013e+00  3.50756412e+01 -2.36580738e+00 -1.17082967e+01\n -1.33060238e+00  4.96365973e+00  2.08790938e+00  4.96191552e+00\n -3.64131198e+01 -3.78497195e+01  1.81503872e+01 -6.21116578e+00\n  1.25491534e+01 -2.75099597e+00 -3.02655330e+00 -9.92613994e+00\n -9.54360697e-01 -1.92846336e+00 -2.04902834e+00 -2.56927769e+00\n -5.00843118e-01 -2.04743606e+00  1.97258659e+00 -5.95709694e-01\n -1.31159335e+00 -1.72467764e+00 -5.79658671e-01 -1.48989959e+00\n -5.64015491e-02 -8.11163152e-01 -1.18025372e+00 -2.69683717e+00\n -6.47426698e-01 -4.81385876e-01  4.76883758e+01  1.11015760e+02\n -3.08580814e+01 -1.10430354e+01  4.13236106e+01 -9.09754253e+00\n  8.59027470e+01 -3.16705208e+01 -3.50782910e+00 -1.60478595e+03\n -1.07701702e+02 -4.26099734e+00 -2.38819173e+00 -1.89308283e+01\n -2.44718770e+00  1.24523442e+01 -3.04570477e+02  1.81530967e+02\n -1.77189590e+01 -3.56811208e+02  1.51228435e+03 -4.41486117e+01\n -7.96317018e+00 -1.37046749e+01 -2.06624238e+00 -3.88181241e-01\n -7.17806967e-02 -4.09474312e+01  3.26906846e+02  2.28355404e+01\n -3.64090742e+01 -2.76685186e+00  4.32299771e-01 -2.32253991e+00\n -3.39058425e+00 -2.10734425e+00 -2.50709568e+00 -1.72061158e+00\n -5.36369716e-01 -1.71899092e+00 -3.63440459e+00 -2.07578850e+00\n -1.10980259e+00 -9.53481562e-01 -6.68438039e-01 -5.82008047e-01\n -2.35628010e+00 -5.31098394e-01 -2.91669905e-01 -1.68230337e+00\n -1.21440311e+00 -4.11739485e-01 -5.74693659e-01 -4.98252215e-01\n -6.18152760e-01 -7.39876378e-01 -9.15180119e-01 -4.41785194e-01\n -2.75346887e-01 -2.22763603e-01 -1.86838808e+00 -1.58341680e+00\n -1.83703125e+00 -1.83052253e+00 -5.41264927e+00 -1.90832376e+00\n -5.80084292e+00 -1.85263553e+00 -6.50054156e+00 -1.11627618e+01\n -1.79820599e+01  7.81050134e+00 -6.09313355e+00 -2.04682018e+00\n -2.56658929e+00 -2.51435555e+00 -1.56426606e+00 -1.15213757e+00\n -1.66253267e+00 -2.24400125e+00 -7.38228847e-01 -9.73891926e-01\n -2.03419666e+00 -8.94479117e-01 -9.46873572e-01 -2.91091599e+00\n -1.79510781e+00 -1.54586637e+00 -1.52932456e+00 -1.33925457e+00\n -6.40507636e-01 -1.26051377e+00 -2.10752821e+00 -1.13675765e+00\n -1.08532384e+00 -1.01531571e+00 -9.30229505e-01 -1.06252181e+00\n -9.17259115e-01 -1.40447083e+00 -1.20843656e+00 -2.82394022e+00\n -2.57197155e+00 -3.51065395e+00 -1.55630444e-01 -2.33892448e+00\n -1.25889309e+00 -2.54387062e+00 -1.48254334e+00 -4.82590388e-01\n -2.15062950e+00 -1.10937005e-01 -2.74851666e-01 -1.46026060e+00\n -6.03994703e-01 -2.78749233e+00 -2.83233280e+00 -4.33020425e+00\n -2.96670844e+00 -2.55834455e+02 -2.91336098e+02  1.03318933e+03\n -6.07809156e+00 -2.92697336e+00 -8.72915981e+01 -2.44498701e+01\n -1.07896232e+00 -3.75198995e+00 -9.90429650e-01 -1.09718878e+00\n -3.84860446e+00 -9.21252496e-01 -1.79660674e+00 -3.60864242e+00\n -9.50512657e-01 -2.45174029e+01  2.37585883e+00  2.49258398e+00\n -1.63845390e+00 -3.79041296e+01 -6.14494467e+00 -2.69407736e+00\n -1.97598851e+00 -2.78304691e+00 -2.01908648e+00 -2.70887776e+00\n -2.27622647e+00 -3.03134136e+01 -2.80535187e+00 -3.32610683e-01\n -1.60111766e+00 -3.49805988e-01 -3.38071186e+00 -3.43652317e-01\n -1.35367716e+00  1.03884747e+02  5.83248178e+00  2.45330915e+01\n -8.29424302e-01  5.74677437e+00 -7.77027182e+01 -2.31249953e+02\n -1.06820919e+00].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# produce probabilities\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_predicted_proba \u001b[38;5;241m=\u001b[39m \u001b[43mLR_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# y_predicted_proba\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# y_predicted_proba[:,1]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_predicted_proba[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1670\u001b[0m, in \u001b[0;36mLogisticRegression.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1665\u001b[0m ovr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124movr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1666\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_class \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1667\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1668\u001b[0m )\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ovr:\n\u001b[1;32m-> 1670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_proba_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1672\u001b[0m     decision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:439\u001b[0m, in \u001b[0;36mLinearClassifierMixin._predict_proba_lr\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict_proba_lr\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;124;03m\"\"\"Probability estimation for OvR logistic regression.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \n\u001b[0;32m    435\u001b[0m \u001b[38;5;124;03m    Positive class probabilities are computed as\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m    1. / (1. + np.exp(-self.decision_function(X)));\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m    multiclass is handled by normalizing that over all classes.\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m     expit(prob, out\u001b[38;5;241m=\u001b[39mprob)\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m prob\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:407\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03mPredict confidence scores for samples.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m    this class would be predicted.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    405\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 407\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:769\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    770\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    771\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    772\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    773\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    774\u001b[0m         )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# make sure we actually converted to numeric:\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[-4.10316093e+01 -3.11294753e+01 -7.49156139e+01 -3.17650901e+00\n -7.57019359e-02 -1.61407726e+01 -1.76669476e+01 -7.88195053e+00\n -2.30037579e+01  3.41914665e+01 -5.45982666e+01 -6.12619774e+01\n -2.73893518e+00 -2.40282052e+01 -4.24838459e+00 -6.78487687e+01\n -6.80796596e+01 -5.69238045e+01 -1.13104029e+02  8.97187487e+00\n -9.14385944e+00  2.76375286e+00  1.02992075e-01 -5.06466180e+00\n -8.43917167e+00 -4.95493355e+00 -2.01295949e+01 -7.25879649e+00\n -2.44824574e+01 -7.52967779e+00 -4.21515316e+01 -6.80091502e+00\n -4.31178902e+01 -2.51901071e+01 -4.14942584e+01 -2.19079096e+02\n -1.96051262e+02 -6.57562171e+01 -2.82917802e+00 -1.38167606e+00\n -5.71286922e+01 -5.91807529e+01 -1.73937402e+01 -6.03188001e+01\n -6.09747298e-01 -3.09081875e+01 -7.31797179e+01 -2.57436269e+00\n -1.08627620e+02 -1.02399836e+02 -1.11074175e+02 -2.26493241e+02\n -2.86061197e+02 -5.16976586e+01 -3.24513383e-01 -5.91145926e+01\n -4.05972685e+01 -1.34723483e+01 -2.01306083e+00 -4.23139809e+00\n -3.84754573e+00 -6.09215626e+01 -5.46475432e-02 -3.47818110e+01\n -6.50097006e+01 -3.33925082e+00 -1.16497549e+01 -3.78217082e+01\n -1.99264901e+01  2.61400362e+01 -3.75875301e+00 -3.26843434e+01\n  9.41723418e+01 -3.45828917e+01  1.82762949e+01 -6.69343323e+00\n -1.56714388e+01 -3.38977840e+02 -4.17218233e+01 -4.87279529e+01\n -3.37839082e+01 -4.23123793e+00 -2.26530547e+01 -4.63202319e+00\n -6.80008709e+01  1.26317994e+02 -8.97216448e+03  1.28975197e+04\n  6.29013074e+00 -9.17502967e-01  5.52813047e+00 -5.39347631e+03\n -1.91184413e+03  5.19742846e+00  6.60420512e+01 -3.95053589e+02\n  6.85150165e+01 -1.28394190e+02  4.20138850e-02  1.29749465e+00\n -1.28690941e+01 -6.17977909e+01 -1.52735259e+01 -3.57290982e+02\n -9.06234214e+00 -3.90304895e+01 -1.30725279e+02 -5.42155042e+01\n -1.93197750e+02 -3.62801822e+00 -2.03201234e+02 -1.36865321e+02\n -2.58715857e+00 -1.57388741e+01 -2.13554771e+00 -1.36247759e+01\n  1.02177466e+00 -2.42560889e+01  3.76595527e+03 -4.51970768e+01\n -1.54040277e+02 -1.30785552e+01 -2.06504426e+00 -2.59545508e+00\n -2.34626480e+00 -4.58844646e+00 -1.59249003e+00 -5.17916309e-02\n -1.89081689e+00 -2.90818285e+02 -4.17034327e+00 -3.30107473e+01\n -2.75170039e+00 -9.23776875e+00 -4.46438150e-01 -1.42928974e+00\n -4.55227430e-01 -5.22512485e-02 -4.86438377e+01 -1.38781680e+00\n -5.87077468e-01  6.09995257e+00 -8.97616918e-01 -9.67277763e-01\n -9.59337183e-01 -7.32507428e-01 -7.35829751e+00 -4.92985133e-01\n -1.53735123e+00 -1.60514073e+00 -7.93229598e-01 -5.81531207e-01\n -6.55677066e-01 -5.59355417e+00 -1.34347462e+00 -7.75361107e+00\n  8.54465449e-02 -7.75306538e+00 -1.35929688e+01 -2.03798983e+01\n  6.21319431e+00 -5.83205450e+00  1.34439285e+00 -2.57100720e+00\n -3.71308137e+00 -2.71796659e+00 -1.01938410e+00 -2.01479888e+00\n -1.99985487e+00 -1.08145796e+00 -5.73194796e-01 -1.98293851e+00\n  5.58015979e-01 -7.01134119e-01 -2.09277747e+00 -1.68820393e+00\n -7.18697613e-01 -1.70875354e+00 -5.72414940e-02 -8.12620552e-01\n -1.16881877e+00 -2.63226170e+00  1.87377296e+01  3.56539745e+01\n -1.52288851e+01 -6.65661296e+00  4.77243593e+00 -3.53599610e+00\n  4.85032750e+01 -1.53342640e+01 -2.91338090e+00 -4.68651190e+02\n -3.98538187e+01 -1.47571751e+00 -2.24390442e+00 -7.20564301e+00\n -2.44558880e+00  6.66395036e+00 -6.62663390e+01  3.49255398e+01\n  5.39393011e+02 -1.21870189e+01 -1.57277972e+00 -2.01858165e+00\n -2.46080775e-01 -7.25012819e-02 -6.77989253e+00  9.51433359e+01\n  5.64371927e+00 -1.95352433e+01 -2.82984534e+00 -2.27011764e+00\n -3.58284144e+00 -2.65903728e+00 -2.32668693e+00 -5.13074681e+00\n -1.60647991e+00 -5.20037638e-02 -2.06267098e+00 -1.89612768e+03\n -1.89578954e+01 -1.48998813e+02  4.30814142e+01 -2.76711024e+01\n -4.63226833e-01  1.00245554e-01 -4.44867206e-01 -5.28670227e-02\n -1.43646130e+02 -1.35638527e+00 -8.45553589e-01  1.14434280e+01\n -8.40887555e-01 -8.93602758e-01 -9.50396721e-01 -7.26305723e-01\n -1.13915724e+01 -6.23076809e-01 -2.09467857e+00  1.99013418e-01\n  1.29358013e+00  3.50756412e+01 -2.36580738e+00 -1.17082967e+01\n -1.33060238e+00  4.96365973e+00  2.08790938e+00  4.96191552e+00\n -3.64131198e+01 -3.78497195e+01  1.81503872e+01 -6.21116578e+00\n  1.25491534e+01 -2.75099597e+00 -3.02655330e+00 -9.92613994e+00\n -9.54360697e-01 -1.92846336e+00 -2.04902834e+00 -2.56927769e+00\n -5.00843118e-01 -2.04743606e+00  1.97258659e+00 -5.95709694e-01\n -1.31159335e+00 -1.72467764e+00 -5.79658671e-01 -1.48989959e+00\n -5.64015491e-02 -8.11163152e-01 -1.18025372e+00 -2.69683717e+00\n -6.47426698e-01 -4.81385876e-01  4.76883758e+01  1.11015760e+02\n -3.08580814e+01 -1.10430354e+01  4.13236106e+01 -9.09754253e+00\n  8.59027470e+01 -3.16705208e+01 -3.50782910e+00 -1.60478595e+03\n -1.07701702e+02 -4.26099734e+00 -2.38819173e+00 -1.89308283e+01\n -2.44718770e+00  1.24523442e+01 -3.04570477e+02  1.81530967e+02\n -1.77189590e+01 -3.56811208e+02  1.51228435e+03 -4.41486117e+01\n -7.96317018e+00 -1.37046749e+01 -2.06624238e+00 -3.88181241e-01\n -7.17806967e-02 -4.09474312e+01  3.26906846e+02  2.28355404e+01\n -3.64090742e+01 -2.76685186e+00  4.32299771e-01 -2.32253991e+00\n -3.39058425e+00 -2.10734425e+00 -2.50709568e+00 -1.72061158e+00\n -5.36369716e-01 -1.71899092e+00 -3.63440459e+00 -2.07578850e+00\n -1.10980259e+00 -9.53481562e-01 -6.68438039e-01 -5.82008047e-01\n -2.35628010e+00 -5.31098394e-01 -2.91669905e-01 -1.68230337e+00\n -1.21440311e+00 -4.11739485e-01 -5.74693659e-01 -4.98252215e-01\n -6.18152760e-01 -7.39876378e-01 -9.15180119e-01 -4.41785194e-01\n -2.75346887e-01 -2.22763603e-01 -1.86838808e+00 -1.58341680e+00\n -1.83703125e+00 -1.83052253e+00 -5.41264927e+00 -1.90832376e+00\n -5.80084292e+00 -1.85263553e+00 -6.50054156e+00 -1.11627618e+01\n -1.79820599e+01  7.81050134e+00 -6.09313355e+00 -2.04682018e+00\n -2.56658929e+00 -2.51435555e+00 -1.56426606e+00 -1.15213757e+00\n -1.66253267e+00 -2.24400125e+00 -7.38228847e-01 -9.73891926e-01\n -2.03419666e+00 -8.94479117e-01 -9.46873572e-01 -2.91091599e+00\n -1.79510781e+00 -1.54586637e+00 -1.52932456e+00 -1.33925457e+00\n -6.40507636e-01 -1.26051377e+00 -2.10752821e+00 -1.13675765e+00\n -1.08532384e+00 -1.01531571e+00 -9.30229505e-01 -1.06252181e+00\n -9.17259115e-01 -1.40447083e+00 -1.20843656e+00 -2.82394022e+00\n -2.57197155e+00 -3.51065395e+00 -1.55630444e-01 -2.33892448e+00\n -1.25889309e+00 -2.54387062e+00 -1.48254334e+00 -4.82590388e-01\n -2.15062950e+00 -1.10937005e-01 -2.74851666e-01 -1.46026060e+00\n -6.03994703e-01 -2.78749233e+00 -2.83233280e+00 -4.33020425e+00\n -2.96670844e+00 -2.55834455e+02 -2.91336098e+02  1.03318933e+03\n -6.07809156e+00 -2.92697336e+00 -8.72915981e+01 -2.44498701e+01\n -1.07896232e+00 -3.75198995e+00 -9.90429650e-01 -1.09718878e+00\n -3.84860446e+00 -9.21252496e-01 -1.79660674e+00 -3.60864242e+00\n -9.50512657e-01 -2.45174029e+01  2.37585883e+00  2.49258398e+00\n -1.63845390e+00 -3.79041296e+01 -6.14494467e+00 -2.69407736e+00\n -1.97598851e+00 -2.78304691e+00 -2.01908648e+00 -2.70887776e+00\n -2.27622647e+00 -3.03134136e+01 -2.80535187e+00 -3.32610683e-01\n -1.60111766e+00 -3.49805988e-01 -3.38071186e+00 -3.43652317e-01\n -1.35367716e+00  1.03884747e+02  5.83248178e+00  2.45330915e+01\n -8.29424302e-01  5.74677437e+00 -7.77027182e+01 -2.31249953e+02\n -1.06820919e+00].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# produce probabilities\n",
    "y_predicted_proba = LR_model.predict_proba(sc.transform(X_test))\n",
    "# y_predicted_proba\n",
    "# y_predicted_proba[:,1]\n",
    "print(y_predicted_proba[0])\n",
    "print(\"Chance of Failure is \", 100*y_predicted_proba[0,1], \"%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to Another Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare to the Naive Bayes Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for # Naive Bayes Classifier\n",
      "[[ 19   3]\n",
      " [223  69]]\n",
      "\n",
      "Average model prediction accuracy using Naive Bayes Classifier: 34.14%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare with Naive Bayes Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train, y_train)\n",
    "NB_scores = cross_val_score(nb_clf, X_train, y_train, scoring='accuracy', cv=5)\n",
    "NB_avg_score = np.average(NB_scores)\n",
    "\n",
    "# make a prediction\n",
    "y_test_pred_nb = nb_clf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_test_pred_nb)\n",
    "print(\"Confusion matrix for # Naive Bayes Classifier\")\n",
    "print(cm)\n",
    "print('\\nAverage model prediction accuracy using Naive Bayes Classifier: {:.2%}\\n'.format(NB_avg_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model is an accurate model with an average prediction accuracy of 91.09%.  \n",
    "\n",
    "We compared to the results of the Naive Bayes Classifier, and the logistic regressor is much more accurate in this application."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data_preprocessing_tools.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 357.5,
   "position": {
    "height": "379.5px",
    "left": "1161px",
    "right": "20px",
    "top": "122px",
    "width": "319px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "124e47c1ad6ceb91c3ceb1dde65cfb37b91587009ebc216d439d411f4704dda1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
